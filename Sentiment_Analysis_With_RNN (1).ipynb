{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis With RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpN5DMURn-oi"
      },
      "source": [
        "# PREPARING THE MOVIE REVIEW DATA\n",
        "# sentiment_data = https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "import tarfile\n",
        "with tarfile.open('/content/aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
        "  tar.extractall()\n",
        "\n",
        "# Preprocessing the movie dataset into a more convenient format\n",
        "import pyprind\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# change the 'basepath' to the directory of the unzipped movie dataset\n",
        "basepath = '/content/aclImdb'\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "df = pd.DataFrame()\n",
        "for s in ('test', 'train'):\n",
        "  for l in ('pos', 'neg'):\n",
        "    path = os.path.join(basepath, s, l)\n",
        "    for file in sorted(os.listdir(path)):\n",
        "      with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
        "        txt = infile.read()\n",
        "      df = df.append([[txt, labels[l]]], ignore_index = True)\n",
        "      pbar.update()\n",
        "\n",
        "df.columns = ['review', 'sentiment']\n",
        "\n",
        "# Storing the movie review dataset as a CSV file\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head(3)\n",
        "#df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyaD4F_4oFqZ"
      },
      "source": [
        "# STEP 1: CREATE A DATASET\n",
        "target = df.pop('sentiment')\n",
        "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "    (df.values, target.values)\n",
        ")\n",
        "\n",
        "# INSPECTION\n",
        "for ex in ds_raw.take(3):\n",
        "  print(ex[0].numpy()[0][ :50], ex[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHtbWfX4oHRE"
      },
      "source": [
        "# SPLITTING THE DATASET INTO TRAINING, TESTING & VALIDATION\n",
        "tf.random.set_seed(1)\n",
        "ds_raw = ds_raw.shuffle(\n",
        "    50000, reshuffle_each_iteration=False\n",
        ")\n",
        "ds_raw_test = ds_raw.take(25000)\n",
        "ds_raw_train_valid = ds_raw.skip(25000)\n",
        "ds_raw_train = ds_raw_train_valid.take(20000)\n",
        "ds_raw_valid = ds_raw_train_valid.skip(20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtXM3GfEoI1T"
      },
      "source": [
        "# STEP 2: FIND UNIQUE TOKENS (WORDS)\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "token_counts = Counter()\n",
        "\n",
        "for example in ds_raw_train:\n",
        "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
        "    token_counts.update(tokens)\n",
        "    \n",
        "print('Vocab-size:', len(token_counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPmSQLoqoKbb"
      },
      "source": [
        "# STEP 3: ENCODING UNIQUE TOKENS TO INTEGERS\n",
        "encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
        "example_str = 'This is an example!'\n",
        "print(encoder.encode(example_str), '\\n')\n",
        "\n",
        "# STEP 3A: DEFINE THE FUNCTION FOR TRANSFORMATION\n",
        "def encode(text_tensor, label):\n",
        "  text = text_tensor.numpy()[0]\n",
        "  encoded_text = encoder.encode(text)\n",
        "  return encoded_text, label\n",
        "\n",
        "# STEP 3B: WRAP THE ENCODE FUNCTION TO A TF Op.\n",
        "def encode_map_fn(text, label):\n",
        "  return tf.py_function(encode, inp=[text, label], \n",
        "                        Tout = (tf.int64, tf.int64))\n",
        "  \n",
        "ds_train = ds_raw_train.map(encode_map_fn)\n",
        "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
        "ds_test = ds_raw_test.map(encode_map_fn)\n",
        "\n",
        "# looking at the shape of some examples:\n",
        "tf.random.set_seed(1)\n",
        "for example in ds_train.shuffle(1000).take(5):\n",
        "  print('Sequence length:', example[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_8rrlLoL6h"
      },
      "source": [
        "# Making the length of the sequence equal\n",
        "# Take a small subset\n",
        "ds_subset = ds_train.take(8)\n",
        "for ex in ds_subset:\n",
        "  print('Individual size:', ex[0].shape)\n",
        "\n",
        "print('\\n')\n",
        "# Dividing the dataset into batches\n",
        "ds_batched = ds_subset.padded_batch(\n",
        "    4, padded_shapes=([-1], [])\n",
        ")\n",
        "for batch in ds_batched:\n",
        "  print('Batch dimension:', batch[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKV99JMboNZv"
      },
      "source": [
        "# Dividing the datasets into mini-batches with a batch-size of 32\n",
        "train_data = ds_train.padded_batch(\n",
        "    32, padded_shapes=([-1], [])\n",
        ")\n",
        "valid_data = ds_valid.padded_batch(\n",
        "    32, padded_shapes=([-1], [])\n",
        ")\n",
        "test_data = ds_test.padded_batch(\n",
        "    32, padded_shapes=([-1], [])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA7m9G17oOqw"
      },
      "source": [
        "embedding_dim = 20\n",
        "vocab_size = len(token_counts) + 2\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# Build the model\n",
        "bi_lstm_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(\n",
        "          input_dim=vocab_size,\n",
        "          output_dim=embedding_dim,\n",
        "          name='embed-layer'\n",
        "      ),\n",
        "\n",
        "      tf.keras.layers.Bidirectional(\n",
        "          tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
        "          name='bidir-lstm'\n",
        "      ),\n",
        "\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "bi_lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt383gnpoQbP"
      },
      "source": [
        "# Compile and train\n",
        "bi_lstm_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = bi_lstm_model.fit(\n",
        "    train_data, \n",
        "    validation_data=valid_data,\n",
        "    epochs=15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1-EYlV-oRw8"
      },
      "source": [
        "# Evaluate\n",
        "test_results = bi_lstm_model.evaluate(test_data)\n",
        "print('Test Acc: {:.2f}%'.format(test_results[1] * 100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}